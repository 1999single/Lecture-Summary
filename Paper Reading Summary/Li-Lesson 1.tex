%% By WeijiWeijieLi

### NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis

​	This paper proposes that a sparse set of input views can be used to optimize a potential continuous volume scene function. The neural radiance fields is introduced to represent scene, which is a fully-connected deep neural network. Input a 5D coordinate into the network, and the output will be the volume density and the radiance related to the scene on the corresponding coordinate. Obviously, this 5D coordinate constitutes the concept of a 5D scene, which is a key factor in view synthesis. Different from the traditional 3D coordinates, the observation direction coordinates (θ, φ) are introduced into the 5D coordinates. Based on my current knowledge, it is like observing a point. Observing from multiple angles will more truly restore the real scene than observing from a single angle. For view synthesis, it should be the same.  

​	Generate 5D coordinates with the help of camera rays, and use classic volume rendering techniques to generate 2D images. Of course, in order to reduce the error between the synthetic view and the real view, the entire process needs to use gradient descent to optimize the model. The author found that it is necessary to use positional encoding to transform the input 5D coordinates so that the multi-layer perceptron (MLP) can represent higher frequency functions. For this reason, they proposed a layered sampling process to improve the sampling efficiency of rendering. The current results have proved that the use of 5D neural radiance fields to represent the scene produces better results than traditional methods. However, the research on the effective optimization and rendering of the technology of the neural radiance fields still needs to be improved. When writing the weight of a scene in a deep neural network, we still don't know how to solve the problem of interpretability. 

### Neural Volume Rendering NeRF And Beyond

​	I understand this paper as an explanation and discussion of the above. In this paper, the neural volume rendering is explained once again: the method of generating images or videos by tracking the rays that enter the scene and performing certain integrals on the length of the rays. This is just a very popular explanation, and it has been explained in more detail above. This article will focus on the explanation, performance and application of neural volume rendering.  

​	First, four papers that use neural networks as scalar function approximators to define occupancy or signed distance functions are introduced. They are from 2019 CVPR and 2019 ICCV. There is also a series of articles based on the idea of implicit functions.  

​	Next, the author mentioned an article by Lombardi et al. in *the Neural Volumes paper*, which introduced volume rendering of view synthesis. Although it still achieved some success, it was still based on voxels identification. Perhaps the author's intention is to form an example of comparison with NeRF. So the following discussion goes to NeRF itself. One of the reasons NeRF is able to render very detailed is that it uses a periodic activation function to encode 3D points and related view directions on the rays. This innovation was later promoted to form SIREN (Sinusoidal Representation Network). But NeRF also has shortcomings: 1. Its training and rendering speed are too slow. 2. It can only represent static scenes. 3. There are requirements for lighting conditions. 4. The trained NeRF model cannot be applied to other scenes or objects. In response to this series of shortcomings, the author also gave some articles on improving them.  

​	Although neural volume rendering and NeRF-style articles have achieved some success, it is still unknown whether it will be competent in the end. Because there are still many interference factors in the real world. The author writes this article and hopes that those who are interested in research in this field can help, and hope that readers can maintain objective judgments. 



### Neural Body: Implicit Neural Representations with Structured Latent Codes for Novel View Synthesis of Dynamic Humans

​	As mentioned in *Neural Volume Rendering NeRF And Beyond*, NeRF has excellent view synthesis capabilities. However, by representing the 3D scene as an implicit field of density and color, it relies on a highly dense input views on the one hand, and can only be applied to a static scene on the other hand. This paper proposes an implicit neural representation method based on structural latent code (Neural Body), which aims to solve the problem of sparse view synthesis and new view synthesis of dynamic humans. This technology can be applied to free-view video systems, and has low hardware requirements to reduce the cost of free-view video systems. The core of Neural Body is to anchor a set of latent codes on the vertices of the deformable human human model. In other words, their spatial positions transform the code positions according to the human body posture. Then, based on these latent codes, a network is designed to regress the density and color of any 3D point. This solves the shortcomings of the Lombardi team's research that the latent code is obtained independently, which leads to the lack of sufficient constraints for fusion across frames. The contributions of this paper are as follows: 1. The author proposes a new method that can synthesize new realistic views of performers in complex motion from sparse multi-viewpoint videos. 2. The author proposes Neural Body, a new type of implicit neural representation for dynamic human bodies, which enables us to effectively merge observations from video frames. 3. Compared with the previous work, the author's method has a significant performance improvement.



###  Neural Volumes: Learning Dynamic Renderable Volumes from Images

​	This paper is dedicated to solving the problem of modeling and rendering of dynamic scenes. In the face of complex scenes, traditional grid-based reconstruction and tracking methods are difficult to work. This paper proposes a learning-based method to represent dynamic objects. The unique feature is that this method is based on 2D image supervision in a multi-view capture design and does not require explicit reconstruction or tracking of objects. This method consists of an encoder-encoder network that converts the input image into a 3D volume representation and a distinguishable ray advance operation. In addition, the distortion field needs to be used to realize a dynamic irregular grid structure during the ray traveling. This structure can improve the apparent resolution and reduce grid-like artifacts and jagged motion. Ultimately, this technology can be used to help improve the problem of low interaction between objects in virtual reality modeling near-field scenes and users. Of course, the actual application scenarios are by no means limited to VR. 



### Mixture of Volumetric Primitives for Efficient Neural Rendering

​	In this paper, the author proposes the concept of Mixture Volumetric Primitives(MVP), which is mainly applied to human dynamic rendering. This is a representation used to render dynamic 3D content, which combines a representation based on volume and primitives. Although the NeRF proposed by Mildenhall et al. solves the resolution problem of compact representation, it can only be used to process static scenes. At the same time, the rendering time of MLP is too long when synthesizing a single high-resolution image. In this case, MVP can have a better performance. It solves the memory and calculation limitations of existing volumetric methods. This is a new voxel mesh motion model that can better cope with dynamic scenes. Moreover, it can train faster and render the learning model in real time. 

